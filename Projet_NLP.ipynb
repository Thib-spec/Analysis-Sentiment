{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet_NLP",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXX_l8e79nYx"
      },
      "source": [
        "# Analyse de sentiments à l'aide d'un réseau  RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT-YLEDq9_up"
      },
      "source": [
        "# Importation des fichiers et librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kljOthG4vksA",
        "outputId": "162ebe03-ab61-4e68-e790-ae1987116264"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5blobtXMyMJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f712b3de-c6e3-4cb4-d062-56ae0af7b6aa"
      },
      "source": [
        "!pip install trax"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting trax\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/51/305b839f51d53abb393777f743e497d27bb341478f3fdec4d6ddaccc9fb5/trax-1.3.7-py2.py3-none-any.whl (521kB)\n",
            "\r\u001b[K     |▋                               | 10kB 14.5MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20kB 19.8MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 12.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |███▏                            | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |███▊                            | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |████▍                           | 71kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 81kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 102kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 112kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 122kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 133kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 143kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 153kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 163kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 174kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 184kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 194kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 204kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 215kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 225kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 235kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 245kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 256kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 266kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 276kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 286kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 296kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 307kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 317kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 327kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 337kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 348kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 358kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 368kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 378kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 389kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 399kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 409kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 419kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 430kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 440kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 450kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 460kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 471kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 481kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 491kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 501kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 512kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 522kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trax) (1.19.5)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (from trax) (0.1.60+cuda101)\n",
            "Collecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/c0/c0fed4301f592c3b56638ae7292612c17d91a43891ba1aaf9636d535beae/tensorflow_text-2.4.3-cp37-cp37m-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 14.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from trax) (0.10.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from trax) (4.0.1)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from trax) (0.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from trax) (1.4.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from trax) (5.4.8)\n",
            "Collecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c6/2ea21c983ae27553a798829a533349de5df99678cfd3fd8d313ae30b063f/t5-0.8.1-py3-none-any.whl (214kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 37.5MB/s \n",
            "\u001b[?25hCollecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (from trax) (0.2.9)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib->trax) (1.12)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (0.11.0)\n",
            "Requirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (2.4.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.3.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (3.12.4)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (20.3.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (5.1.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.28.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (4.41.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.16.0)\n",
            "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/20/23bbc94034e16bb1ace73e9e7922226e31d6d36b88dcfa257d2c59b3f465/mesh_tensorflow-0.1.18-py3-none-any.whl (361kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 37.9MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from t5->trax) (3.2.5)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 38.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from t5->trax) (1.7.1+cu101)\n",
            "Collecting transformers>=2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from t5->trax) (0.22.2.post1)\n",
            "Collecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/43/83a554ba7679b00faf283204e5fb603eb378e098c64008bafe7e73f3371a/tfds_nightly-4.2.0.dev202102270106-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 36.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from t5->trax) (2.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from t5->trax) (1.1.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.10.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.32.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.4.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.36.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.3.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.7.4.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->trax) (53.0.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->trax) (3.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.52.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2020.12.5)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/82/22/e684c9e2e59b561dbe36538852e81849122c666c423448e3a5c99362c228/portalocker-2.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (3.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 43.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->t5->trax) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->t5->trax) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->t5->trax) (2.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.27.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=2.7.0->t5->trax) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.7.0->t5->trax) (7.1.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=4f12a4745e8a302aebecf7ad21fdbd06be6e2528ddfee0c7fb38d92bd6a42848\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tensorflow-text, mesh-tensorflow, rouge-score, portalocker, sacrebleu, sentencepiece, tokenizers, sacremoses, transformers, tfds-nightly, t5, funcsigs, trax\n",
            "Successfully installed funcsigs-1.0.2 mesh-tensorflow-0.1.18 portalocker-2.2.1 rouge-score-0.0.4 sacrebleu-1.5.0 sacremoses-0.0.43 sentencepiece-0.1.95 t5-0.8.1 tensorflow-text-2.4.3 tfds-nightly-4.2.0.dev202102270106 tokenizers-0.10.1 transformers-4.3.3 trax-1.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5H009Yjx-LM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c7f4a3-8cb2-418f-ee8b-da99a34ed158"
      },
      "source": [
        "import os\r\n",
        "import nltk\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download(\"stopwords\")\r\n",
        "\r\n",
        "\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.stem.lancaster import LancasterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "\r\n",
        "from keras.layers import Dense, Embedding, LSTM, Activation\r\n",
        "\r\n",
        "import keras\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQgh1OsE-lVE"
      },
      "source": [
        "#Pré-traitement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yUFNkI3IyjxZ",
        "outputId": "30c8f2f9-66da-4e6b-866d-d3b84949e0b3"
      },
      "source": [
        "data = pd.read_csv(\"gdrive/MyDrive/Projet_NLP/sample_data.csv\",encoding = \"ISO-8859-1\",header = None)\r\n",
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  ...                                                  5\n",
              "0  0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1  0  ...  is upset that he can't update his Facebook by ...\n",
              "2  0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3  0  ...    my whole body feels itchy and like its on fire \n",
              "4  0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xRjkyUf_A0K"
      },
      "source": [
        "### 1-1- Sélection des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "41Bxpa8u1hY4",
        "outputId": "206a44b4-b6e1-4a18-c282-8677528c98d3"
      },
      "source": [
        "data_to_begin = (data[799990:800090])   # On sélectionne un échantillon de données contenant des reviews positives et négatives\r\n",
        "data_to_begin"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>799990</th>\n",
              "      <td>0</td>\n",
              "      <td>2329204651</td>\n",
              "      <td>Thu Jun 25 10:28:26 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Badmantalking</td>\n",
              "      <td>and I think it has a mind of its own,like it a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799991</th>\n",
              "      <td>0</td>\n",
              "      <td>2329204705</td>\n",
              "      <td>Thu Jun 25 10:28:27 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>loveisnothing</td>\n",
              "      <td>@iaintnohomo  Banana will be playing later. BU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799992</th>\n",
              "      <td>0</td>\n",
              "      <td>2329204790</td>\n",
              "      <td>Thu Jun 25 10:28:27 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>CJROSE218</td>\n",
              "      <td>@koolgirl37 read my tweet below</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799993</th>\n",
              "      <td>0</td>\n",
              "      <td>2329204835</td>\n",
              "      <td>Thu Jun 25 10:28:27 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattfca</td>\n",
              "      <td>My life  http://mattf.ca/2009/06/24/yay-me/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799994</th>\n",
              "      <td>0</td>\n",
              "      <td>2329204987</td>\n",
              "      <td>Thu Jun 25 10:28:28 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>360cookie</td>\n",
              "      <td>Tried to get the mutant Fawkes to follow me bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800085</th>\n",
              "      <td>4</td>\n",
              "      <td>1467825594</td>\n",
              "      <td>Mon Apr 06 22:23:38 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>emmasaur28</td>\n",
              "      <td>eating ice-cream with chocolate topping</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800086</th>\n",
              "      <td>4</td>\n",
              "      <td>1467825631</td>\n",
              "      <td>Mon Apr 06 22:23:38 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Rezdamc</td>\n",
              "      <td>@iamdiddy your telling me.. just finished doin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800087</th>\n",
              "      <td>4</td>\n",
              "      <td>1467825641</td>\n",
              "      <td>Mon Apr 06 22:23:41 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>laurengonewildx</td>\n",
              "      <td>@its_yvonne daaang i didn't know it was possib...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800088</th>\n",
              "      <td>4</td>\n",
              "      <td>1467825712</td>\n",
              "      <td>Mon Apr 06 22:23:42 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Andrael</td>\n",
              "      <td>@LaurUy West.  Mostly San Jose and San Francisco.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800089</th>\n",
              "      <td>4</td>\n",
              "      <td>1467825726</td>\n",
              "      <td>Mon Apr 06 22:23:40 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Eric_John</td>\n",
              "      <td>Nikki is off working her ass off on a bike, tr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0  ...                                                  5\n",
              "799990  0  ...  and I think it has a mind of its own,like it a...\n",
              "799991  0  ...  @iaintnohomo  Banana will be playing later. BU...\n",
              "799992  0  ...                   @koolgirl37 read my tweet below \n",
              "799993  0  ...        My life  http://mattf.ca/2009/06/24/yay-me/\n",
              "799994  0  ...  Tried to get the mutant Fawkes to follow me bu...\n",
              "...    ..  ...                                                ...\n",
              "800085  4  ...           eating ice-cream with chocolate topping \n",
              "800086  4  ...  @iamdiddy your telling me.. just finished doin...\n",
              "800087  4  ...  @its_yvonne daaang i didn't know it was possib...\n",
              "800088  4  ...  @LaurUy West.  Mostly San Jose and San Francisco.\n",
              "800089  4  ...  Nikki is off working her ass off on a bike, tr...\n",
              "\n",
              "[100 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3moKThKEKBTr",
        "outputId": "24c413ac-7fc5-46bd-e4cf-271740b05688"
      },
      "source": [
        "reviews = []\r\n",
        "fit_reviews = []\r\n",
        "for i in data_to_begin.iloc(1)[0]:          # On créé une liste contenant le caractère positif (4) ou négatif (0)\r\n",
        "  reviews.append(i)\r\n",
        "\r\n",
        "for i in reviews:\r\n",
        "  if i == 4:\r\n",
        "    fit_reviews.append(1)                   # On modifie les 4 en 1 pour notre modèle\r\n",
        "  else:\r\n",
        "    fit_reviews.append(0)\r\n",
        "print(fit_reviews)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8BY3YEZx9yh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJQ48DBx_zS9"
      },
      "source": [
        "### 1-2 Tokenisation, StopWords et Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jDd_3b5LoV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "712f25f7-e398-43e0-ab69-fde67f56b217"
      },
      "source": [
        "tweet_content = []                                                              \r\n",
        "                                                                                \r\n",
        "content = []\r\n",
        "tokenizer = nltk.RegexpTokenizer(r'\\w+')\r\n",
        "english_stopwords = nltk.corpus.stopwords.words(\"english\")\r\n",
        "\r\n",
        "stemmer = LancasterStemmer()\r\n",
        "\r\n",
        "for i in data_to_begin.iloc(1)[5]:\r\n",
        "  tweet_content.append(tokenizer.tokenize(i.lower()))                           # On tokenize nos mots\r\n",
        "\r\n",
        "\r\n",
        "tweet_content_without_stopwords = []\r\n",
        "pos_or_neg_reviews = []\r\n",
        "\r\n",
        "for i in tweet_content:\r\n",
        "  tweet_sw = []\r\n",
        "  for j in i:\r\n",
        "    if j not in english_stopwords:\r\n",
        "      tweet_sw.append(stemmer.stem(j))                                          # On retire les stopwords et on effectue le stemming\r\n",
        "  tweet_content_without_stopwords.append([tweet_sw,data_to_begin.loc(1)[0]])\r\n",
        "\r\n",
        "\r\n",
        "for j in range(len(tweet_content_without_stopwords)):\r\n",
        "  content.append(tweet_content_without_stopwords[j][0])\r\n",
        "\r\n",
        "print(content)                                                                  # content contient le contenu des tweets avec le pré-traitement effectué\r\n"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['think', 'mind', 'lik', 'alway', 'ring', 'tak', 'leak', 'jeez', 'mak', 'whol', 'thing', 'embarrass', 'mus'], ['iaintnohomo', 'banan', 'play', 'lat', 'shel', 'play', 'caro'], ['koolgirl37', 'read', 'tweet'], ['lif', 'http', 'mattf', 'ca', '2009', '06', '24', 'yay'], ['tri', 'get', 'mut', 'fawk', 'follow', 'lon', 'without', 'follow', 'nev', 'on'], ['sick', 'spend', 'day', 'lay', 'bed', 'list', 'taylorswift13'], ['gmail'], ['rest', 'peac', 'farrah', 'sad'], ['eric_urbane', 'sound', 'lik', 'riv', 'flag', 'ad', 'much', 'though'], ['resit', 'exam', 'sum', 'wish', 'work', 'hard', 'first', 'year', 'un'], ['lov', 'health4uandpets', 'u', 'guy', 'r', 'best'], ['im', 'meet', 'on', 'besty', 'tonight', 'cant', 'wait', 'girl', 'talk'], ['darealsunisakim', 'thank', 'twit', 'ad', 'sunis', 'got', 'meet', 'hin', 'show', 'dc', 'are', 'sweetheart'], ['sick', 'real', 'cheap', 'hurt', 'much', 'eat', 'real', 'food', 'plu', 'friend', 'mak', 'soup'], ['lovesbrooklyn2', 'effect', 'everyon'], ['productoffear', 'tel', 'burst', 'laugh', 'real', 'loud', 'thank', 'mak', 'com', 'sulk'], ['r_keith_hill', 'than', 'respons', 'ihad', 'already', 'find', 'answ'], ['keepinupwkr', 'jeal', 'hop', 'gre', 'tim', 'vega', 'lik', 'acm', 'lov', 'show'], ['tommcf', 'ah', 'congr', 'mr', 'fletch', 'fin', 'join', 'twit'], ['e4voip', 'respond', 'stupid', 'cat', 'help', 'typ', 'forg', 'er'], ['crazy', 'day', 'school', '10', 'hour', 'straiiight', 'watch', 'hil', 'spencerprat', 'told', 'ha', 'happy', 'birthday', 'jb'], ['naughtyhaughty', 'forget', 'two', 'half', 'men', 'lov', 'show'], ['nileyjileyluv', 'hah', 'worry', 'get', 'hang'], ['soundwav2010', 'least', 'on', 'feel', 'lost', 'may', 'caus', 'many', 'lat', 'us', 'night', 'already', 'addict'], ['lutheranlucciol', 'mak', 'sur', 'dm', 'post', 'link', 'video', 'lt', 'lol', 'gt', 'miss', 'bet', 'get', 'permit', 'bless', 'first'], ['ad', 'twe', 'new', 'iphon'], ['michellard', 'real', 'know', 'think', 'glob', 'yeah', 'san', 'gum', 'na', 'ko', 'par', 'alam', 'ko', 'na', 'din', 'kung', 'makakasam', 'ako'], ['nicolerichy', 'pict', 'sweet'], ['catch', 'email', 'rss', 'random', 'bacn', 'cut', 'ear', 'tonight', '11', '30pm', 'din', 'lauraw'], ['dant', 'around', 'room', 'pjs', 'jam', 'ipod', 'get', 'dizzy', 'wel', 'twit', 'ask'], ['plac', 'peep', 'contest', 'thank', 'vot', 'anyway'], ['going', 'bed', 'goodnight', 'everyon', 'sweet', 'dream', 'http', 'twitp', 'com', '2y2e0'], ['littlelum', 'walk', 'put', 'deposit', 'tomorrow'], ['followinq', 'dachesterfrench', 'shud', 'tha'], ['lordpov', 'meant', 'ad', 'back', 'quot', 'twit', 'toilet', 'cubic', 'somewh', 'quot'], ['aw', 'hold', 'new', 'puppy', 'wel', 'min', 'cuty'], ['ijohn', 'kitteh', 'sleepin', 'crotch', 'prov', 'lik'], ['dramab', 'agree'], ['reach', 'amrits', 'hour', 'find', 'bus', 'wagah', 'bord', '2pm', 'http', 'bkit', 'com', '06fuj'], ['albinl', 'think', 'tonight', 'let', 'first', 'interview', 'fam', 'sup', 'star'], ['happy', 'spend', 'tim', 'famy'], ['fin', 'going', 'bed', 'tir', 'gonn', 'watch', 'hil', 'didnt'], ['eff', 'tir', 'throat', 'hurt', 'ooooohh', 'got', 'crazy', 'crav', 'pin', 'colad', 'banan', 'slushy'], ['deon', 'upload', 'di', 'indowebst', 'dong', 'bangggg'], ['wisdom', 'welcom', 'glad', 'enjoy'], ['hawaii808shellz', 'hahah', 'omg', 'wer', 'laughin', 'hook', 'cuz', 'das', 'roll', 'ryt', 'sheldawg'], ['sickwiththep', 'awww', 'pooky', 'feel', 'bet', 'pray', 'bag', 'nurs'], ['yay', 'found', 'new', 'cuddl', 'buddy'], ['think', 'met', 'first', 'quot', 'snob', 'quot', 'twit', 'tonight', 'bad', 'lif', 'goe'], ['across', 'univers', 'sleep', 'rehears', 'tomorrow'], ['annabel', 'dry', 'sweet', 'potato', 'huh'], ['jonathanrknight', 'hi', 'jon', 'gre', 'hear', 'see', 'cru', 'cannot', 'wait', 'hop', 'wel', 'knight', 'bus', 'lov'], ['long', 'convers', 'mom', 'phon'], ['suitelifeofkel', 'yayyy', 'lol', 'request', 'herr', 'say'], ['pract', 'lin', 'man', 'voic', 'upcom', 'feat', 'shoot', 'prob', 'driv', 'brock', 'crazy'], ['going', 'go', 'read', 'new', 'moon', '3rd', 'tim', 'cant', 'get', 'enough', 'twilight', 'sery'], ['today', 'two', 'mon', 'annivers', 'lov', 'sooooooo', 'much', 'dian', 'omg', 'dont', 'ev', 'know', 'tar', 'heel', 'nca', 'woot'], ['bor'], ['stevecl', 'wallpap', 'red', 'squ', 'ask'], ['r_keith_hill', 'thank', 'respons', 'ihad', 'already', 'find', 'answ'], ['pope_mello', 'wel', 'yeah', 'not', 'cuz', 'bb'], ['tryn', 'get', 'inspir', 'that'], ['kourtneykardash', 'get', 'ear', 'enough', 'night', 'workout', 'much', 'bet'], ['ladygag', 'wait', 'see', 'ur', 'hot', 'ass', 'austin', 'woot', 'woot', 'annnd', 'lov', 'bob', 'purpl', 'went', 'roy', 'col', 'way', 'wel'], ['quotablebuffy', 'got', 'bunch', 'buffy', 'song', 'on', 'fav', 'quot', 'viv', 'quot', 'nerf', 'herd', 'fai', 'met', 'spik', 'buffy', 'body'], ['devun', 'wallpap', 'check', 'gt', 'http', 'twitp', 'com', '2y2e2'], ['morn', 'tweetland', 'long', 'day', 'ahead', 'hop', 'everyon', 'gre', 'day'], ['upload', 'pict', 'friendst'], ['ash_ct', 'aw', 'dont', 'lov', 'lov', 'day', 'thank', 'pic', 'p'], ['noodlebox', 'amand', 'tonight'], ['una_avion2010', 'sorrry', 'lik', '29823782', 'diff', 'thing', 'kevin', 'jona', 'girlfriend'], ['fuzeb', 'sery', 'sing', 'lik', 'who', 'heh', 'lsd', 'mayb', 'j', 'k', 'lolol'], ['lov', 'lif', 'lov'], ['apothecaryjer', 'lov', 'sicil', 'best', 'damn', 'pizz', 'planet', 'say'], ['lov', 'chocol', 'milk', 'gf', 'yeah'], ['going', 'sut', 'creek', 'tomorrow', 'tour', 'old', 'min'], ['bodycoach', 'look', 'cho', 'person', 'lik', 'strawberry', 'going', 'becom', 'coach', 'soon'], ['little__fish', 'guess', 'liv', 'sid', 'ear', 'east', 'mes', 'losiento'], ['paula_paige3489', 'apart', 'sor', 'tryout'], ['fail', 'phys', 'test', 'homo', 'keeen', 'holiday', '2', 'day', 'go'], ['observ', 'upd', 'look', 'alex', 'websit', 'http', 'tinyurl', 'com', 'c48gzf', 'look', 'gre', 'seem', 'alex', 'improv'], ['leiabox', 'tel', 'us', 'tot', 'geek', 'right'], ['theragingoc', 'bonjo', 'spacecowboy', 'wish', 'eith', 'work', 'kid', 'go', 'whenev', 'want', 'go', 'wherev', 'want'], ['rebecca1158', 'goodnight'], ['broth', 'sick', 'lol', 'quit', 'weird'], ['drdrew', 'giv', 'hug', 'cooky', 'hop', 'feel', 'bet'], ['jonathanrknight', 'hey', 'jon', 'real', 'hop', 'tak', 'car', 'want', 'get', 'rundown', 'hug'], ['wrot', 'toilet', 'wal', 'correct', 'poor', 'spel', 'highlight', 'wash', 'hardc'], ['good', 'tech', 'meet', 'clubzon', 'din', 'sush'], ['mattgalloway', 'thank', 'hook', 'carlyrush', 'suggest', 'bro', 'rock'], ['jeffsw', 'depend', 'vert', 'thought', 'on', 'know', 'go', 'lik'], ['speak2ashley', 'hand', 'stil', 'pretty', 'weak', 'cant', 'lik', 'punch', 'anyon', 'yet', 'turn', 'knob', 'doesnt', 'hurt', 'much'], ['honey3223', 'lurk', 'interest'], ['saravananp', 'min', 'b', 'nor', 'stil', 'nee', 'decid', 'aaru', 'hithavaru', 'nin', 'ee', 'moovarol', 'vot'], ['trstfndbby', 'um', 'bought', 'shit', 'kor', 'oach', 'highest', 'qual', 'baby', 'paid', 'extr', '3', '50', '2', 'get', 'real', 'tag', 'sew'], ['eat', 'ic', 'cream', 'chocol', 'top'], ['iamdiddy', 'tel', 'fin', '200', 'crunch', 'step', 'diddy', 'let', 'go'], ['its_yvonne', 'daaang', 'know', 'poss', 'talk', 'long', 'guy', 'talk'], ['lauruy', 'west', 'most', 'san', 'jos', 'san', 'francisco'], ['nikk', 'work', 'ass', 'bik', 'try', 'recov', 'kne', 'injury', 'good']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNcpa1WVAKtj"
      },
      "source": [
        "### 1-3 Définition du vocabulaire"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNueDg50u2-z",
        "outputId": "042d6f24-87ed-4180-b85c-92240810e989"
      },
      "source": [
        "# Building the vocabulary with the train set         (this might take a minute)\r\n",
        "from collections import defaultdict\r\n",
        "\r\n",
        "vocab = defaultdict(lambda: 0)\r\n",
        "vocab['<PAD>'] = 1\r\n",
        "\r\n",
        "for sentence in content:\r\n",
        "  for word in sentence:\r\n",
        "    if word not in vocab:\r\n",
        "      vocab[word] = len(vocab) + 1\r\n",
        "print('The length of the vocabulary is: ', len(vocab))                # Réutilisation de la fonction pour établir le vocabulaire\r\n",
        "print(vocab)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of the vocabulary is:  587\n",
            "defaultdict(<function <lambda> at 0x7f529c1c0290>, {'<PAD>': 1, 'think': 2, 'mind': 3, 'lik': 4, 'alway': 5, 'ring': 6, 'tak': 7, 'leak': 8, 'jeez': 9, 'mak': 10, 'whol': 11, 'thing': 12, 'embarrass': 13, 'mus': 14, 'iaintnohomo': 15, 'banan': 16, 'play': 17, 'lat': 18, 'shel': 19, 'caro': 20, 'koolgirl37': 21, 'read': 22, 'tweet': 23, 'lif': 24, 'http': 25, 'mattf': 26, 'ca': 27, '2009': 28, '06': 29, '24': 30, 'yay': 31, 'tri': 32, 'get': 33, 'mut': 34, 'fawk': 35, 'follow': 36, 'lon': 37, 'without': 38, 'nev': 39, 'on': 40, 'sick': 41, 'spend': 42, 'day': 43, 'lay': 44, 'bed': 45, 'list': 46, 'taylorswift13': 47, 'gmail': 48, 'rest': 49, 'peac': 50, 'farrah': 51, 'sad': 52, 'eric_urbane': 53, 'sound': 54, 'riv': 55, 'flag': 56, 'ad': 57, 'much': 58, 'though': 59, 'resit': 60, 'exam': 61, 'sum': 62, 'wish': 63, 'work': 64, 'hard': 65, 'first': 66, 'year': 67, 'un': 68, 'lov': 69, 'health4uandpets': 70, 'u': 71, 'guy': 72, 'r': 73, 'best': 74, 'im': 75, 'meet': 76, 'besty': 77, 'tonight': 78, 'cant': 79, 'wait': 80, 'girl': 81, 'talk': 82, 'darealsunisakim': 83, 'thank': 84, 'twit': 85, 'sunis': 86, 'got': 87, 'hin': 88, 'show': 89, 'dc': 90, 'are': 91, 'sweetheart': 92, 'real': 93, 'cheap': 94, 'hurt': 95, 'eat': 96, 'food': 97, 'plu': 98, 'friend': 99, 'soup': 100, 'lovesbrooklyn2': 101, 'effect': 102, 'everyon': 103, 'productoffear': 104, 'tel': 105, 'burst': 106, 'laugh': 107, 'loud': 108, 'com': 109, 'sulk': 110, 'r_keith_hill': 111, 'than': 112, 'respons': 113, 'ihad': 114, 'already': 115, 'find': 116, 'answ': 117, 'keepinupwkr': 118, 'jeal': 119, 'hop': 120, 'gre': 121, 'tim': 122, 'vega': 123, 'acm': 124, 'tommcf': 125, 'ah': 126, 'congr': 127, 'mr': 128, 'fletch': 129, 'fin': 130, 'join': 131, 'e4voip': 132, 'respond': 133, 'stupid': 134, 'cat': 135, 'help': 136, 'typ': 137, 'forg': 138, 'er': 139, 'crazy': 140, 'school': 141, '10': 142, 'hour': 143, 'straiiight': 144, 'watch': 145, 'hil': 146, 'spencerprat': 147, 'told': 148, 'ha': 149, 'happy': 150, 'birthday': 151, 'jb': 152, 'naughtyhaughty': 153, 'forget': 154, 'two': 155, 'half': 156, 'men': 157, 'nileyjileyluv': 158, 'hah': 159, 'worry': 160, 'hang': 161, 'soundwav2010': 162, 'least': 163, 'feel': 164, 'lost': 165, 'may': 166, 'caus': 167, 'many': 168, 'us': 169, 'night': 170, 'addict': 171, 'lutheranlucciol': 172, 'sur': 173, 'dm': 174, 'post': 175, 'link': 176, 'video': 177, 'lt': 178, 'lol': 179, 'gt': 180, 'miss': 181, 'bet': 182, 'permit': 183, 'bless': 184, 'twe': 185, 'new': 186, 'iphon': 187, 'michellard': 188, 'know': 189, 'glob': 190, 'yeah': 191, 'san': 192, 'gum': 193, 'na': 194, 'ko': 195, 'par': 196, 'alam': 197, 'din': 198, 'kung': 199, 'makakasam': 200, 'ako': 201, 'nicolerichy': 202, 'pict': 203, 'sweet': 204, 'catch': 205, 'email': 206, 'rss': 207, 'random': 208, 'bacn': 209, 'cut': 210, 'ear': 211, '11': 212, '30pm': 213, 'lauraw': 214, 'dant': 215, 'around': 216, 'room': 217, 'pjs': 218, 'jam': 219, 'ipod': 220, 'dizzy': 221, 'wel': 222, 'ask': 223, 'plac': 224, 'peep': 225, 'contest': 226, 'vot': 227, 'anyway': 228, 'going': 229, 'goodnight': 230, 'dream': 231, 'twitp': 232, '2y2e0': 233, 'littlelum': 234, 'walk': 235, 'put': 236, 'deposit': 237, 'tomorrow': 238, 'followinq': 239, 'dachesterfrench': 240, 'shud': 241, 'tha': 242, 'lordpov': 243, 'meant': 244, 'back': 245, 'quot': 246, 'toilet': 247, 'cubic': 248, 'somewh': 249, 'aw': 250, 'hold': 251, 'puppy': 252, 'min': 253, 'cuty': 254, 'ijohn': 255, 'kitteh': 256, 'sleepin': 257, 'crotch': 258, 'prov': 259, 'dramab': 260, 'agree': 261, 'reach': 262, 'amrits': 263, 'bus': 264, 'wagah': 265, 'bord': 266, '2pm': 267, 'bkit': 268, '06fuj': 269, 'albinl': 270, 'let': 271, 'interview': 272, 'fam': 273, 'sup': 274, 'star': 275, 'famy': 276, 'tir': 277, 'gonn': 278, 'didnt': 279, 'eff': 280, 'throat': 281, 'ooooohh': 282, 'crav': 283, 'pin': 284, 'colad': 285, 'slushy': 286, 'deon': 287, 'upload': 288, 'di': 289, 'indowebst': 290, 'dong': 291, 'bangggg': 292, 'wisdom': 293, 'welcom': 294, 'glad': 295, 'enjoy': 296, 'hawaii808shellz': 297, 'hahah': 298, 'omg': 299, 'wer': 300, 'laughin': 301, 'hook': 302, 'cuz': 303, 'das': 304, 'roll': 305, 'ryt': 306, 'sheldawg': 307, 'sickwiththep': 308, 'awww': 309, 'pooky': 310, 'pray': 311, 'bag': 312, 'nurs': 313, 'found': 314, 'cuddl': 315, 'buddy': 316, 'met': 317, 'snob': 318, 'bad': 319, 'goe': 320, 'across': 321, 'univers': 322, 'sleep': 323, 'rehears': 324, 'annabel': 325, 'dry': 326, 'potato': 327, 'huh': 328, 'jonathanrknight': 329, 'hi': 330, 'jon': 331, 'hear': 332, 'see': 333, 'cru': 334, 'cannot': 335, 'knight': 336, 'long': 337, 'convers': 338, 'mom': 339, 'phon': 340, 'suitelifeofkel': 341, 'yayyy': 342, 'request': 343, 'herr': 344, 'say': 345, 'pract': 346, 'lin': 347, 'man': 348, 'voic': 349, 'upcom': 350, 'feat': 351, 'shoot': 352, 'prob': 353, 'driv': 354, 'brock': 355, 'go': 356, 'moon': 357, '3rd': 358, 'enough': 359, 'twilight': 360, 'sery': 361, 'today': 362, 'mon': 363, 'annivers': 364, 'sooooooo': 365, 'dian': 366, 'dont': 367, 'ev': 368, 'tar': 369, 'heel': 370, 'nca': 371, 'woot': 372, 'bor': 373, 'stevecl': 374, 'wallpap': 375, 'red': 376, 'squ': 377, 'pope_mello': 378, 'not': 379, 'bb': 380, 'tryn': 381, 'inspir': 382, 'that': 383, 'kourtneykardash': 384, 'workout': 385, 'ladygag': 386, 'ur': 387, 'hot': 388, 'ass': 389, 'austin': 390, 'annnd': 391, 'bob': 392, 'purpl': 393, 'went': 394, 'roy': 395, 'col': 396, 'way': 397, 'quotablebuffy': 398, 'bunch': 399, 'buffy': 400, 'song': 401, 'fav': 402, 'viv': 403, 'nerf': 404, 'herd': 405, 'fai': 406, 'spik': 407, 'body': 408, 'devun': 409, 'check': 410, '2y2e2': 411, 'morn': 412, 'tweetland': 413, 'ahead': 414, 'friendst': 415, 'ash_ct': 416, 'pic': 417, 'p': 418, 'noodlebox': 419, 'amand': 420, 'una_avion2010': 421, 'sorrry': 422, '29823782': 423, 'diff': 424, 'kevin': 425, 'jona': 426, 'girlfriend': 427, 'fuzeb': 428, 'sing': 429, 'who': 430, 'heh': 431, 'lsd': 432, 'mayb': 433, 'j': 434, 'k': 435, 'lolol': 436, 'apothecaryjer': 437, 'sicil': 438, 'damn': 439, 'pizz': 440, 'planet': 441, 'chocol': 442, 'milk': 443, 'gf': 444, 'sut': 445, 'creek': 446, 'tour': 447, 'old': 448, 'bodycoach': 449, 'look': 450, 'cho': 451, 'person': 452, 'strawberry': 453, 'becom': 454, 'coach': 455, 'soon': 456, 'little__fish': 457, 'guess': 458, 'liv': 459, 'sid': 460, 'east': 461, 'mes': 462, 'losiento': 463, 'paula_paige3489': 464, 'apart': 465, 'sor': 466, 'tryout': 467, 'fail': 468, 'phys': 469, 'test': 470, 'homo': 471, 'keeen': 472, 'holiday': 473, '2': 474, 'observ': 475, 'upd': 476, 'alex': 477, 'websit': 478, 'tinyurl': 479, 'c48gzf': 480, 'seem': 481, 'improv': 482, 'leiabox': 483, 'tot': 484, 'geek': 485, 'right': 486, 'theragingoc': 487, 'bonjo': 488, 'spacecowboy': 489, 'eith': 490, 'kid': 491, 'whenev': 492, 'want': 493, 'wherev': 494, 'rebecca1158': 495, 'broth': 496, 'quit': 497, 'weird': 498, 'drdrew': 499, 'giv': 500, 'hug': 501, 'cooky': 502, 'hey': 503, 'car': 504, 'rundown': 505, 'wrot': 506, 'wal': 507, 'correct': 508, 'poor': 509, 'spel': 510, 'highlight': 511, 'wash': 512, 'hardc': 513, 'good': 514, 'tech': 515, 'clubzon': 516, 'sush': 517, 'mattgalloway': 518, 'carlyrush': 519, 'suggest': 520, 'bro': 521, 'rock': 522, 'jeffsw': 523, 'depend': 524, 'vert': 525, 'thought': 526, 'speak2ashley': 527, 'hand': 528, 'stil': 529, 'pretty': 530, 'weak': 531, 'punch': 532, 'anyon': 533, 'yet': 534, 'turn': 535, 'knob': 536, 'doesnt': 537, 'honey3223': 538, 'lurk': 539, 'interest': 540, 'saravananp': 541, 'b': 542, 'nor': 543, 'nee': 544, 'decid': 545, 'aaru': 546, 'hithavaru': 547, 'nin': 548, 'ee': 549, 'moovarol': 550, 'trstfndbby': 551, 'um': 552, 'bought': 553, 'shit': 554, 'kor': 555, 'oach': 556, 'highest': 557, 'qual': 558, 'baby': 559, 'paid': 560, 'extr': 561, '3': 562, '50': 563, 'tag': 564, 'sew': 565, 'ic': 566, 'cream': 567, 'top': 568, 'iamdiddy': 569, '200': 570, 'crunch': 571, 'step': 572, 'diddy': 573, 'its_yvonne': 574, 'daaang': 575, 'poss': 576, 'lauruy': 577, 'west': 578, 'most': 579, 'jos': 580, 'francisco': 581, 'nikk': 582, 'bik': 583, 'try': 584, 'recov': 585, 'kne': 586, 'injury': 587})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx8p4wy9-VEJ",
        "outputId": "598dd788-ba61-48ff-a8ad-2fc8029bc7b8"
      },
      "source": [
        "maxlen = 0    \r\n",
        "for word in content:\r\n",
        " if (len(word)) > maxlen:\r\n",
        "   maxlen = len(word)\r\n",
        "\r\n",
        "print(maxlen)                         # On détermine la longueur du mot le plus grand parmi notre échantillon de mot"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M0IexVwActv"
      },
      "source": [
        "###1-4 Mise en place du padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsAFBxXC4LcA",
        "outputId": "98993a00-cb7f-4684-d1e2-10e67495d9cc"
      },
      "source": [
        "padding = []\r\n",
        "for word in content:\r\n",
        "   L = []\r\n",
        "   for i in range(maxlen):\r\n",
        "     L.append(vocab['<PAD>'])\r\n",
        "   for j in range (len(word)):\r\n",
        "     L[j] = vocab[word[j]]                  # On transforme nos mots en vecteur de nombre pour les insérer dans notre modèle\r\n",
        "   padding.append(L)\r\n",
        "\r\n",
        "print(padding) "
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1, 1, 1, 1, 1], [15, 16, 17, 18, 19, 17, 20, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [21, 22, 23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [24, 25, 26, 27, 28, 29, 30, 31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [32, 33, 34, 35, 36, 37, 38, 36, 39, 40, 1, 1, 1, 1, 1, 1, 1, 1], [41, 42, 43, 44, 45, 46, 47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [49, 50, 51, 52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [53, 54, 4, 55, 56, 57, 58, 59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [60, 61, 62, 63, 64, 65, 66, 67, 68, 1, 1, 1, 1, 1, 1, 1, 1, 1], [69, 70, 71, 72, 73, 74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [75, 76, 40, 77, 78, 79, 80, 81, 82, 1, 1, 1, 1, 1, 1, 1, 1, 1], [83, 84, 85, 57, 86, 87, 76, 88, 89, 90, 91, 92, 1, 1, 1, 1, 1, 1], [41, 93, 94, 95, 58, 96, 93, 97, 98, 99, 10, 100, 1, 1, 1, 1, 1, 1], [101, 102, 103, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [104, 105, 106, 107, 93, 108, 84, 10, 109, 110, 1, 1, 1, 1, 1, 1, 1, 1], [111, 112, 113, 114, 115, 116, 117, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [118, 119, 120, 121, 122, 123, 4, 124, 69, 89, 1, 1, 1, 1, 1, 1, 1, 1], [125, 126, 127, 128, 129, 130, 131, 85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [132, 133, 134, 135, 136, 137, 138, 139, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [140, 43, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 1, 1, 1, 1], [153, 154, 155, 156, 157, 69, 89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [158, 159, 160, 33, 161, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [162, 163, 40, 164, 165, 166, 167, 168, 18, 169, 170, 115, 171, 1, 1, 1, 1, 1], [172, 10, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 33, 183, 184, 66, 1, 1], [57, 185, 186, 187, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [188, 93, 189, 2, 190, 191, 192, 193, 194, 195, 196, 197, 195, 194, 198, 199, 200, 201], [202, 203, 204, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [205, 206, 207, 208, 209, 210, 211, 78, 212, 213, 198, 214, 1, 1, 1, 1, 1, 1], [215, 216, 217, 218, 219, 220, 33, 221, 222, 85, 223, 1, 1, 1, 1, 1, 1, 1], [224, 225, 226, 84, 227, 228, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [229, 45, 230, 103, 204, 231, 25, 232, 109, 233, 1, 1, 1, 1, 1, 1, 1, 1], [234, 235, 236, 237, 238, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [239, 240, 241, 242, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [243, 244, 57, 245, 246, 85, 247, 248, 249, 246, 1, 1, 1, 1, 1, 1, 1, 1], [250, 251, 186, 252, 222, 253, 254, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [255, 256, 257, 258, 259, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [260, 261, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [262, 263, 143, 116, 264, 265, 266, 267, 25, 268, 109, 269, 1, 1, 1, 1, 1, 1], [270, 2, 78, 271, 66, 272, 273, 274, 275, 1, 1, 1, 1, 1, 1, 1, 1, 1], [150, 42, 122, 276, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [130, 229, 45, 277, 278, 145, 146, 279, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [280, 277, 281, 95, 282, 87, 140, 283, 284, 285, 16, 286, 1, 1, 1, 1, 1, 1], [287, 288, 289, 290, 291, 292, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [293, 294, 295, 296, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 1, 1, 1, 1, 1, 1, 1], [308, 309, 310, 164, 182, 311, 312, 313, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [31, 314, 186, 315, 316, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 317, 66, 246, 318, 246, 85, 78, 319, 24, 320, 1, 1, 1, 1, 1, 1, 1], [321, 322, 323, 324, 238, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [325, 326, 204, 327, 328, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [329, 330, 331, 121, 332, 333, 334, 335, 80, 120, 222, 336, 264, 69, 1, 1, 1, 1], [337, 338, 339, 340, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [341, 342, 179, 343, 344, 345, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 140, 1, 1, 1, 1, 1, 1, 1], [229, 356, 22, 186, 357, 358, 122, 79, 33, 359, 360, 361, 1, 1, 1, 1, 1, 1], [362, 155, 363, 364, 69, 365, 58, 366, 299, 367, 368, 189, 369, 370, 371, 372, 1, 1], [373, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [374, 375, 376, 377, 223, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [111, 84, 113, 114, 115, 116, 117, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [378, 222, 191, 379, 303, 380, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [381, 33, 382, 383, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [384, 33, 211, 359, 170, 385, 58, 182, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [386, 80, 333, 387, 388, 389, 390, 372, 372, 391, 69, 392, 393, 394, 395, 396, 397, 222], [398, 87, 399, 400, 401, 40, 402, 246, 403, 246, 404, 405, 406, 317, 407, 400, 408, 1], [409, 375, 410, 180, 25, 232, 109, 411, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [412, 413, 337, 43, 414, 120, 103, 121, 43, 1, 1, 1, 1, 1, 1, 1, 1, 1], [288, 203, 415, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [416, 250, 367, 69, 69, 43, 84, 417, 418, 1, 1, 1, 1, 1, 1, 1, 1, 1], [419, 420, 78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [421, 422, 4, 423, 424, 12, 425, 426, 427, 1, 1, 1, 1, 1, 1, 1, 1, 1], [428, 361, 429, 4, 430, 431, 432, 433, 434, 435, 436, 1, 1, 1, 1, 1, 1, 1], [69, 24, 69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [437, 69, 438, 74, 439, 440, 441, 345, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [69, 442, 443, 444, 191, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [229, 445, 446, 238, 447, 448, 253, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [449, 450, 451, 452, 4, 453, 229, 454, 455, 456, 1, 1, 1, 1, 1, 1, 1, 1], [457, 458, 459, 460, 211, 461, 462, 463, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [464, 465, 466, 467, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [468, 469, 470, 471, 472, 473, 474, 43, 356, 1, 1, 1, 1, 1, 1, 1, 1, 1], [475, 476, 450, 477, 478, 25, 479, 109, 480, 450, 121, 481, 477, 482, 1, 1, 1, 1], [483, 105, 169, 484, 485, 486, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [487, 488, 489, 63, 490, 64, 491, 356, 492, 493, 356, 494, 493, 1, 1, 1, 1, 1], [495, 230, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [496, 41, 179, 497, 498, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [499, 500, 501, 502, 120, 164, 182, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [329, 503, 331, 93, 120, 7, 504, 493, 33, 505, 501, 1, 1, 1, 1, 1, 1, 1], [506, 247, 507, 508, 509, 510, 511, 512, 513, 1, 1, 1, 1, 1, 1, 1, 1, 1], [514, 515, 76, 516, 198, 517, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [518, 84, 302, 519, 520, 521, 522, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [523, 524, 525, 526, 40, 189, 356, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [527, 528, 529, 530, 531, 79, 4, 532, 533, 534, 535, 536, 537, 95, 58, 1, 1, 1], [538, 539, 540, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [541, 253, 542, 543, 529, 544, 545, 546, 547, 548, 549, 550, 227, 1, 1, 1, 1, 1], [551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 474, 33, 93, 564, 565], [96, 566, 567, 442, 568, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [569, 105, 130, 570, 571, 572, 573, 271, 356, 1, 1, 1, 1, 1, 1, 1, 1, 1], [574, 575, 189, 576, 82, 337, 72, 82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [577, 578, 579, 192, 580, 192, 581, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [582, 64, 389, 583, 584, 585, 586, 587, 514, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXRNeaJ_9gBd"
      },
      "source": [
        "#Création d'une Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E70NsU_d3kFE"
      },
      "source": [
        "simple_model = tf.keras.models.Sequential()\r\n",
        "\r\n",
        "simple_model.add(tf.keras.layers.SimpleRNN(15, input_shape=(1000,1000)))   # On définit une première version d'un modèle RNN simple\r\n",
        "\r\n",
        "simple_model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(0.01))                 "
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SscFt72a387f"
      },
      "source": [
        "simple_model.fit(padding,fit_reviews,epochs = 10)  # Il y a une incompatibilité entre les données et le paramètre input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVU89hOp9hCh"
      },
      "source": [
        "# Création d'un modèle plus complexe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1I9tdjzDYXw"
      },
      "source": [
        "# model = tf.keras.models.Sequential()\r\n",
        "\r\n",
        "# model.add(tf.keras.layers.Embedding(input_dim= 5000, output_dim= 5000))   \r\n",
        "# model.add(tf.keras.layers.Bidirectional(\r\n",
        "#     tf.keras.layers.LSTM(5)\r\n",
        "# ))\r\n",
        "# model.add(tf.keras.layers.Dense(6, activation='relu'))\r\n",
        "# model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYD70KanOsy6"
      },
      "source": [
        "model = tf.keras.models.Sequential()\r\n",
        "model.add(tf.keras.layers.Embedding(input_dim= 1000, output_dim= 1000))         # Les choix des paramètres seront expliqués dans le rapport\r\n",
        "                                                                                # On compléxifie notre modèle\r\n",
        "model.add(tf.keras.layers.LSTM(15))\r\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(0.01))"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKLCeTRk6lUt"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R_ThOMlHoGk",
        "outputId": "39835861-a105-49c7-ecf7-98cb963a0380"
      },
      "source": [
        "model.fit(padding,fit_reviews,epochs = 100)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1928\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.1193\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0712\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0457\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0302\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0199\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0119\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0078\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.0057\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0044\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0036\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0029\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0024\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0020\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0018\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0016\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0015\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0014\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0014\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0013\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0013\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0012\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0012\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0011\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0011\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0011\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0010\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 9.9247e-04\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 9.6090e-04\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 9.3179e-04\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 9.0352e-04\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 8.7776e-04\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 8.5197e-04\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 8.2788e-04\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 8.0483e-04\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 7.8335e-04\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 7.6161e-04\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 7.4145e-04\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 7.2155e-04\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 7.0230e-04\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 6.8392e-04\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 6.6607e-04\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 6.4966e-04\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 6.3333e-04\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 6.1739e-04\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 6.0209e-04\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 5.8830e-04\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 5.7496e-04\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 5.6233e-04\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 5.5045e-04\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 5.3927e-04\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 5.2832e-04\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 5.1804e-04\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 5.0828e-04\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 4.9868e-04\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 4.8934e-04\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 4.8070e-04\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 4.7210e-04\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 4.6361e-04\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 4.5562e-04\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 4.4744e-04\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 4.3984e-04\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 4.3218e-04\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 4.2467e-04\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 4.1783e-04\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 4.1079e-04\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 4.0414e-04\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 3.9777e-04\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 3.9141e-04\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 3.8483e-04\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 3.7839e-04\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 3.7235e-04\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 3.6659e-04\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 3.6050e-04\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 3.5484e-04\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 3.4891e-04\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 3.4291e-04\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 3.3692e-04\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 3.3108e-04\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 3.2440e-04\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 3.1534e-04\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 3.0740e-04\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 3.0155e-04\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 2.9689e-04\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 2.9252e-04\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 2.8809e-04\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 2.8375e-04\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 2.7968e-04\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 2.7542e-04\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 2.7145e-04\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 2.6759e-04\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 2.6388e-04\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 2.6032e-04\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 2.5688e-04\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 2.5340e-04\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 2.4996e-04\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 2.4670e-04\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 2.4329e-04\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 2.4013e-04\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 2.3698e-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f529b2d8710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNHeLAx6ma1i"
      },
      "source": [
        "model.save_weights(\"gdrive/MyDrive/Projet_NLP/model\")"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOCZZ_QCWPA4",
        "outputId": "acf8e5d2-bc2c-4d9a-ccc2-9367d2a5f5f3"
      },
      "source": [
        "print(model.predict(np.expand_dims(padding[2] , axis = 0))) "
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f52971cc710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[[0.00065884]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNgy4ntkA4gx"
      },
      "source": [
        "# Mise en place de la Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb3gTuoMmXMc"
      },
      "source": [
        "def analyse_sentiment(sentence):\r\n",
        "  content = []\r\n",
        "  tokenizer = nltk.RegexpTokenizer(r'\\w+')\r\n",
        "  english_stopwords = nltk.corpus.stopwords.words(\"english\")\r\n",
        "\r\n",
        "  stemmer = LancasterStemmer()\r\n",
        "  phrase = tokenizer.tokenize(sentence.lower())\r\n",
        "  for i in phrase:\r\n",
        "    if i not in english_stopwords:\r\n",
        "      content.append(stemmer.stem(i))\r\n",
        "  #print(content) \r\n",
        "\r\n",
        "\r\n",
        "  maxlen = 0\r\n",
        "  for word in content:\r\n",
        "    if (len(word)) > maxlen:\r\n",
        "      maxlen = len(word)\r\n",
        "  #pint(maxlen) \r\n",
        "\r\n",
        "  from collections import defaultdict\r\n",
        "\r\n",
        "  vocab = defaultdict(lambda: 0)\r\n",
        "  vocab['<PAD>'] = 1\r\n",
        "\r\n",
        "  for word in content:\r\n",
        "    if word not in vocab:\r\n",
        "      vocab[word] = len(vocab) + 1\r\n",
        "  print(vocab)\r\n",
        "\r\n",
        "\r\n",
        "  padding = []\r\n",
        "  for word in content:\r\n",
        "    if word in vocab:\r\n",
        "      padding.append(vocab[word])                           # Si le mot appartient au vecteur de vocabulaire on ajoute l'indice correspondant sinon on ajoute un 0\r\n",
        "    else:\r\n",
        "      padding.append(0)\r\n",
        "\r\n",
        "  print(padding)\r\n",
        "\r\n",
        "  model.load_weights(\"gdrive/MyDrive/Projet_NLP/model\")\r\n",
        "  result = model.predict(np.expand_dims(padding ,axis = 0))\r\n",
        "  print(result)\r\n",
        "\r\n",
        "  if result <= 0.5:\r\n",
        "    print(\"Your sentence is negative\")                      # Il y a un problème dans la prédiction du modèle où la valeur reste inférieure à 0.5 bien que le modèle précédent soit fonctionnel\r\n",
        "  else:\r\n",
        "    print(\"Your sentence is positive\")\r\n",
        "\r\n"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEbeQU73o8W8",
        "outputId": "79dbc15d-6935-4433-d4db-8248ebb1d6d1"
      },
      "source": [
        "analyse_sentiment(\"I love mondays\")"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function analyse_sentiment.<locals>.<lambda> at 0x7f529c1b4d40>, {'<PAD>': 1, 'lov': 2, 'monday': 3})\n",
            "[2, 3]\n",
            "WARNING:tensorflow:6 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f52971cc710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[[0.0911974]]\n",
            "Your sentence is negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikU4y5jnyCZr"
      },
      "source": [
        "# Appel à l'API yelp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApF-5HqSc8JR",
        "outputId": "3a81cd3b-b329-4507-edf7-7a8ac4f6ef09"
      },
      "source": [
        "!pip install yelp\r\n",
        "!pip install requests"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: yelp in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from yelp) (1.15.0)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.7/dist-packages (from yelp) (0.17.4)\n",
            "Requirement already satisfied: oauth2 in /usr/local/lib/python3.7/dist-packages (from yelp) (1.9.0.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woou9ijBFBb8"
      },
      "source": [
        "import requests\r\n",
        "import json\r\n",
        " \r\n",
        "api_key='nCPX_SfMsQJne-Y6mmZsJNI2uYH53uIDavpQSIc1wbz2YsoCT5c5sWObDqyoZra9b2fWeeaKHv2bcBKdYGx6eXE4xyZUM18HUw2cSVaB4t4nzdZrYs1O5t9ZCvI4YHYx'\r\n",
        "headers = {'Authorization': 'Bearer %s' % api_key}\r\n",
        "\r\n",
        "url = \"https://api.yelp.com/v3/businesses/FEVQpbOPOwAPNIgO7D3xxw/reviews\"   # On effectue la requête à l'API\r\n",
        "req = requests.get(url, headers=headers)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmQsQtYCGkjc",
        "outputId": "57356ac6-afc7-41b8-bdfe-757d17140049"
      },
      "source": [
        "parsed = json.loads(req.text)  # On récupére le contenu du fichier json\r\n",
        "whole_data = parsed['reviews']\r\n",
        "yelp_reviews = []\r\n",
        "index = 0\r\n",
        "for i in whole_data:\r\n",
        "  yelp_reviews.append(whole_data[index]['text'])  # On sélectionne uniquement l'avis du consommateur\r\n",
        "  index +=1\r\n",
        "\r\n",
        "print(yelp_reviews)   # yelp_reviews contient bien les différentes reviews"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Truth be told if it was up to me I'd be giving 4/5 stars, we did order recently and our cheese fries weren't delivered, couldn't speak with anyone from...\", 'Happened to be in the city today and not too far from here so I had to stop by and pick up cheese fries (I love those crinkle cuts!) and a shake. Yum. Tbh,...', 'Before I moved to NY, my first ever trip to the city was in 2005 - this was when Shake Shack had this one lone original location, and it was an event to go...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh1oIsZ9H2X5",
        "outputId": "fb8a5a39-499f-4648-d2f5-bc449308f42e"
      },
      "source": [
        "analyse_sentiment(yelp_reviews[1])"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function analyse_sentiment.<locals>.<lambda> at 0x7f52971fad40>, {'<PAD>': 1, 'hap': 2, 'city': 3, 'today': 4, 'far': 5, 'stop': 6, 'pick': 7, 'chees': 8, 'fri': 9, 'lov': 10, 'crinkl': 11, 'cut': 12, 'shak': 13, 'yum': 14, 'tbh': 15})\n",
            "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
            "WARNING:tensorflow:7 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f52971cc710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[[0.07961369]]\n",
            "Your sentence is negative\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}